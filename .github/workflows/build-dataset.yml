name: Build Dataset

on:
  workflow_dispatch:
    inputs:
      turns:
        type: number
        default: 30
  schedule:
    - cron: '0 3 * * *'

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT_TOKEN }}
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Cache deps
        uses: actions/cache@v4
        with:
          key: python-${{ hashFiles('requirements.txt') }}
          path: |
            ~/.cache/pip
            ~/.cache/torch/sentence_transformers
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # 追加: パッケージを editable でインストール
          pip install -e .
      - name: Scrape tech docs
        run: python scripts/scrape_docs.py --out tech.jsonl
      - name: Generate dialogs
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ST_CACHE: ${{ github.workspace }}/.cache/st
        PYTHONPATH: ${{ github.workspace }}
        run: |
          # 1) Q&A 生成 → dialogs.csv を runs/<exp_id>/ に保存
          python facade/collector.py --auto -n 50 \
          --q-provider openai --ai-provider openai \
          --quiet --summary \
          --output dialogs.csv
          # 2) 直近の runs ディレクトリを取得して変換
          run_dir=$(ls -td runs/* | head -n 1)
          python scripts/csv_to_jsonl.py "$run_dir/dialogs.csv" dialogs.jsonl
      - name: Auto tag
        run: |
          python scripts/classify_domain.py --inp tech.jsonl --out tech_tagged.jsonl --domain tech
          python scripts/classify_domain.py --inp dialogs.jsonl --out dialogs_tagged.jsonl --domain general
      - name: Merge dataset
        run: |
          python scripts/merge_to_dataset.py tech_tagged.jsonl dialogs_tagged.jsonl daily.jsonl
      - name: Upload current.csv as artefact
        uses: actions/upload-artifact@v4
        with:
          name: current-dataset-csv
          path: datasets/current.csv

      - name: Commit dataset              # ⇦ 置き換え
        id: commit
        run: |
          set -e
          DATE=$(date +%Y%m%d)
          mkdir -p dataset
          cp daily.jsonl dataset/$DATE.jsonl

          git config --global user.name  'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'

          # --- pull 最新 main (自ブランチ) を反映 -----------------------------
          # 自分の変更を一時退避してからリベースで取り込む
          git pull --rebase --autostash origin main

          git add dataset/$DATE.jsonl
          if git diff --cached --quiet; then
            echo "No dataset changes; skipping commit."
            echo "date=$DATE" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          git commit -m "Add dataset for $DATE"
          git push origin HEAD:main

          echo "date=$DATE" >> "$GITHUB_OUTPUT"

      # pull が競合等で失敗すると上記 set -e によりジョブは fail します
      # その場合は手動で main を更新してから再実行してください
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: dataset_today
          path: dataset/${{ steps.commit.outputs.date }}.jsonl
          retention-days: 30

  #----------------------------------------------
  # 追加：再計算 Job  (build が終わってから走る)
  #----------------------------------------------
  recalc:
    needs: build                     # ← artefact を受け取るため依存させる
    runs-on: ubuntu-latest
    env:
      PYTHONPATH: ${{ github.workspace }}
      ST_CACHE:   ${{ github.workspace }}/.cache/st
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          pip install -r requirements.txt
          pip install -e .

      - name: Download dataset artefact
        uses: actions/download-artifact@v4
        with:
          name: current-dataset-csv     # ← build Job で付けた名前
          path: datasets                # datasets/current.csv が展開される

      - name: Recalculate PoR/ΔE v4
        run: |
          python scripts/recalc_scores_v4.py \
            --infile  datasets/current.csv \
            --outfile datasets/current_recalc.parquet

      - name: Upload recalculated dataset
        uses: actions/upload-artifact@v4
        with:
          name: recalculated-dataset
          path: datasets/current_recalc.parquet
